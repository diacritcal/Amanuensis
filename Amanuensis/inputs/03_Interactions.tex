%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\rawhtml
<a name="collabration_communication_value"></a>
\endrawhtml
\subsectional{Interaction: Natural Language Processing}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

The brain didn't evolve to accommodate language, rather, language evolved to accommodate the brain~\cite{ChaterandChristiansenHLB-11}. Biological and cognitive constraints determine what types of linguistic structure are learned, processed and transmitted from person to person and generation to generation. Language acquisition has comparatively little to do with linguistics and is probably best viewed as a form of skill acquisition. Indeed, we are constantly processing streams of sensory information into successively more abstract representations while simultaneously learning to recode the compressed information into hierarchies of skills that serve our diverse purposes~\cite{ChaterandChristiansenCOiBS-18,ChateretalJML-16}.

Contrary to what some textbook authors might think, students learn to code by writing programs, a process that can be considerably accelerated by timely communication with peers and invested collaborators. In the case of unequal skill levels, communication tends to be on the terms of the more capable interlocutor, and the onus of understanding on the less capable partner in the collaboration. To sustain the collaboration, we need to bootstrap the apprentice to achieve a reasonble threshold level of competence in both language and in working with computers so as to compensate for expert programmer's investment in effort. From a value-proposition perspective, the apprentice has to provide net positive benefit to the programmer from day one.

It is important to keep in mind that any program specification whether in the form of input/output pairs or natural language descriptions when communicated between individuals of differing expertise is just the beginning of a conversation. Experts are often careless in specifying computations and assume too much of the student. Students, on the other hand, can surprise experts with their logical thinking while frustrate and disappoint with their difficulty in handling ambiguity and analogy, particularly of the esoteric sort familiar to professional programmers. Input from an expert will typically consist of a stream of facts, suggestions, heuristics, shortcuts, etc., peppered with clarifications, interruptions and other miscellany.

We propose a hybrid system for achieving competence in communicating programming knowledge and collaboratively generating software by creating a non-differentiable conventional dialogue management system that works in tandem with a differentiable neural-network dialogue (NND) system that will become more competent as it gains experience coding and collaborating with its expert partner. The deployment of these two language systems will be controlled on a sentence-by-sentence basis by a meta-reinforcement learning (MRL) system that will depend less and less on the more conventional system but likely never entirely eclipse its utility. The MRL system subsumes the role of a beam search or softmax layer in an encoder-decoder dialogue model. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The conventional system will be built as a hierarchical planner following in the footsteps of the CMU Ravenclaw Dialogue System~\cite{BohusandRudnickyCSL-09} and will come equipped with a relatively sophisticated suite of hierarchical dialogue plans for dealing with communication problems arising due to ambiguity and misunderstanding. While clumsy compared to how humans handle ambiguity and misunderstanding, these dialogue plans are designed to resolve the ambiguity and mitigate the consequences of misunderstanding quickly and get the conversation back on track by attempting various repairs involving requests for definition, clarification, repetition and restatement in as inconspicuous manner as possible~\cite{BohusPhD-07}.

The conventional dialogue system will also include a collection of hierarchical plans for interpreting requests made by the expert programmer to alter programs in the shared editor space, execute programs on specified inputs and perform analyses on the output generated by the program, debugger and other tools provided in the integrated development environment (IDE) accessible through a set of commands implemented as either primitive tasks in the non-differentiable hierarchical planner or through a differentiable neural computer (DNC) interface using the NND system that will ultimately replace most of the functions of the hierarchical-planner-based dialogue management system.

This dual mode dialogue system and its MRL controller allows the apprentice to practice on its own and rewards it for learning to emulate the less flexible hierarchical planner implementation. Indeed there is quite a bit that we can do to preload the apprentice’s basic language competence and facility using the instrumented IDE and related compiler chain. A parallel dialogue system implemented using the same hierarchical planner can be designed to carry out the programmer’s side of the conversation so as to train the NND system and the meta-reinforcement learning system that controls its deployment utterance by utterance. We can also train domain-specific language models using $n$-gram corpora gleaned from discussions between pair programmers engaged in writing code for projects requiring the same programming language and working on similar programming tasks.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In a collaboration, figuring out what to say requires planning and a certain degree of imagination. Suppose you are the apprentice and you want to tell the programmer with whom you're collaborating that you don't understand what a particular expression does. You want to understand what role it plays in the program you are jointly working on. How do you convey this message? What do you need to say explicitly and what can be assumed common knowledge? What does the programmer know and what does she need to be told in order to provide you assistance?

Somehow you need to model what the programmer knows. In planning what to say, you might turn this around and imagine that you're the programmer and ask how you would respond to an apprentice's effort to solicit help, but in imagining this role reversal you have be careful that you don't assume the programmer knows everything that you do. You need a model of what you know as well as a model of what the programmer knows. This is called Theory of Mind (ToM) reasoning and learning how to carry out such reasoning occurs in a critical stage of child development.

Shared knowledge includes general knowledge about programming, knowledge about the current state of a particular program you are working on, as well as specific details concerning what you are attending to at the moment, including program fragments and variable names that have been mentioned recently in the discussion or can be inferred from context. This sort of reasoning can be applied recursively if, for example, the apprentice wants to know what the programmer thinks it knows about what the apprentice knows. To a large extent we can finesse the problem of reasoning about other minds by practicing transparency, redundancy and simplicity so that both parties can depend on not having to work hard to figure out what the other means. However, there are some opportunities in the programmer's apprentice problem for applying ToM reasoning to parts of the problem that cannot be so easily finessed.

Suppose that the apprentice has started a new program using an existing program $P$ following a suggestion by the expert programmer. Realizing that the body of a loop in $P$ is irrelevant to the task at hand, the apprentice replaces the useless body $B$ with a fragment from another program that does more or less what is required and then makes local changes to the fragment to create a new body $B'$ so that it works with the extant loop variables, e.g., loop counter, termination flag, etc. When the assistant has completed these local changes, the programmer intervenes and changes the name of a variable in $B'$. What induced the programmer to make this change?

The programmer noticed that the variable in $B'$ was not initialized or referenced in $P$ but that another variable that was initialized in $P$ and is no longer referenced \emdash{} it only appeared in the original loop body $B$, is perfectly suited for the purposes of the new program. Assume for the sake of this discussion, that the programmer does not explain her action. How might the assistant learn from this intervention or, at the very least, understand why it was made? A reasonable theory of mind might assume that agents perform actions for reasons and those reasons often have to do with preconditions for acting in the world, and, moreover, that determining if action-enabling preconditions are true often requires effort. A useful ToM also depends on having a model allowing an agent to infer how preconditions enable actions by working backward from actions to enabling preconditions. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Imagine the following scene, there's a man holding the reins of a donkey harnessed to a two-wheeled cart \emdash{} often called a {\it{dray}} and its owner referred to as a {\it{drayman}} \emdash{} carrying a load of rocks. He makes the donkey rear up and by so doing the surface of the cart tilts, dumping the rocks onto the road which was clearly his intention given the appreciative nods from the onlooking pedestrians. Aude Oliva used this short {\urlh{./content/Donkey_Cart_Draymans_Quick_Unloading_Trick.mp4}{video}} at the beginning of a talk to illustrate that, while this was, for most of us, an unusual way of delivering a load of rocks, we all believed that we understood exactly how it worked. Not so!

The fact is that, as with so many other perceptual and conceptual tasks, people feel strongly that they perceived or understood much more than in fact they did. For example, most people would be hard-pressed to induce a donkey to rear up and, if you asked them to draw the donkey harnessed to the cart with its load of stone, they would very likely misrepresent the geometric relationships involving the height of the donkey, how the harness is attached, how far off the ground the axle is located, the diameter of the wheels and the level of the cart surface and center of gravity of the load with respect to the axle's frame of reference. In other words, they would not have \emdash{} and possibly never could have \emdash{} designed a working version of the system used by the drayman.

Now imagine that the drayman has a new apprentice who was watching the entire scene with some concentration, anticipating that he might want to do the very same thing before the first week of his apprenticeship is complete. Sure enough, the next day the drayman tells the apprentice to take a load of bricks to a building site in town where they are constructing a chimney on a new house. He stacks the bricks in a pile that looks something like how he remembers the rocks were arranged on the dray the day before. Unfortunately the load isn't balanced over the axle and almost lifts the donkey off its feet. After some experimentation he discovers how to balance the weight so the donkey can pull the load of bricks without too much effort.

When he finally gets to the building site, he nearly gets trampled by the donkey in the process of repeatedly trying to induce the distressed animal to rear up on its hind legs. Finally, one of the brick masons intervenes and demonstrates the trick. Unfortunately, the bricks don’t slide neatly off the dray as the rocks did for the experienced drayman the day before, but instead the bricks on the top of the stack tumble to the pavement and several break into pieces. The helpful brick mason suggests that in the future the assistant should prepare the dray by sprinkling a layer of sand on the surface of cart so that the bricks will slide more freely and that he should also dump the bricks on a softer surface to mitigate possible breakage. He then helps the assistant to unload the rest of the bricks but refuses to pay for the broken ones, telling the assistant he will probably have to pay the drayman to make up for the difference.

An interesting challenge is to develop a model based on what is known about the human brain explaining how memories of the events depicted in the video and extended in the above story might be formed, consolidated, and, subsequently, retrieved, altered, applied and finally assigned a value taking into account the possible negative implications of damaged goods and destroyed property. In the story above, the assistant initially uses his innate "physics engine" to convince himself that he understands the lesson from the master drayman, he then uses a combination of his physical intuitions and trial-and-error to load the cart, but runs up against a wall due to his unfamiliarity with handling reluctant beasts of burden. Finally, he gets into trouble with laws of friction and the quite-reasonable expectations of consumers unwilling to pay for damaged goods. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We don't propose to solve the general problems of theory-of-mind and physics-based reasoning in developing a programmer's apprentice, though the application provides an interesting opportunity to address particular special cases. As mentioned earlier, the stream of conversation between the assistant an expert programmer will inevitably relate to many different topics and specialized areas of expertise. It will include specific and general advice, reasons for acting, suggestions for what to attend to and a wide range of comments and criticisms. 

The apprentice will want to separate this information into different categories to construct solutions to problems that arise at multiple levels of abstraction and complexity during code synthesis. Or will it? We like to think of knowledge neatly packaged into modules that result in textbooks, courses, monographs, tutorials, etc. The apparent order in which activities appear in a stream of activities is largely a consequence of the context in which those activities are carried out. They may seem to arise in accord with some plan, as if assembled and orchestrated with a particular purpose in mind, but, even if there was plan at the outset, we tend to make up things on the fly to accommodate the sort of unpredictable circumstances that characterize most of our evolutionary history. 

In some cases that context or purpose is used to assign a name, but that name or contextual handle is seldom used to initiate or identify the activity except in academic circumstances where divisions and boundaries are highly prized and made much of. The point of this is that in a diverse stream of activities \emdash{} or utterances intended to instigate activities \emdash{} credit assignment can be difficult. Proximity in terms of the length of time or number of intervening activities between a action and a reward is not necessarily a good measure of its value. We suggest it is possible to build a programer's apprentice or other sort of digital assistant that performs its essential services primarily by learning to predict actions, their consequences and their value from observing such a diverse stream of dialog intermixed with actions and observations. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsectional{Interpretabity}

There is a good deal of interest in building AI systems that are said to be {\it{interpretable}}, presumably because we believe interpretability is a property of human intelligence that we would like to emulate in the AI systems that we build and depend upon. There is a substantial literature in cognitive and behavioral neuroscience that suggests humans are not nearly as transparent and rational even at our most thoughtful and least stressed. Nick Chater captures this characteristic nicely in the title of his book, {\it{The Mind is Flat}}, in which he argues that what we generally take as cognitive depth is in fact shallow post hoc reasoning\footnote{%
%
  Nick Chater, Professor of Behavioural Science at the Warwick Business School, believes that most of what we conceive of as cognitive depth is an illusion and that actually we mostly make up things as we go along and generally post hoc. His recent book~\cite{Chater2018} entitled {\it{The Mind is Flat: The Illusion of Mental Depth and The Improvised Mind}} provides an introduction to the literature supporting this hypothesis. I don't entirely agree with him and think he overstates his case. Arguing, as Chater does, that the mind is flat basically assumes there is no {\urlh{https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow}{Type II thinking}}, only Type I~\cite{Kahneman2011}. This is like assuming that reformatting an Excel spreadsheet will always take 14 error-prone steps rather than the one step required to execute a macro that performs the reformatting flawlessly. It assumes we can't listen to a well-argued case for acting contrary to the way we've acted in the past and then successfully change our customary behavior to act in accord with the convincing argument. 

  It assumes we can't overcome our instincts and biases even if we train ourselves to recognize occasions when we might instinctively succumb to those biases and arrange things so that bells and whistles go off in our heads to remind us to sidestep the temptation. Somewhat more charitably, perhaps Chater is saying that in many contexts in which the outcome doesn't matter one way or another we allow ourselves to give into our biases \emdash{} we simply can't be bothered to think more deeply about the alternatives. Or perhaps, being less charitable to all the rest of us, Chater is saying that even in contexts that matter we can't be bothered to do the right thing \emdash{} or at least can't be bothered / aren't disciplined enough to think carefully enough to convince ourselves to do the right thing. I expect quite a number of us have the inclination and resolve to demonstrate that Chater is exaggerating his claim with respect to situations where it makes a difference.}.

If you agree with Chater's hypothesis, it would appear that by building systems modeled after the architecture of the human brain and forced to learn about the world in the same way we do, we will have to either give up on interpretability and transparency at least at the level and to the degree that some believe is necessary, or completely redesign the architectures of the current generation of artificial neural-network AI systems to operate on different principles. I believe that the demand for interpretability constitutes a double standard and is a throwback to the heyday of symbolic AI systems that are brittle in large part because unlike modern neural networks they rely upon categories with crisp boundaries and ostensibly precise, unambiguous semantic interpretations.

The programmer's apprentice is equipped with a powerful physics engine for the world in which it operates, namely the world of computer programs running on von Neumann computing architectures. This engine takes the form of an integrated development environment (IDE) specially instrumented to make it extremely easy for the assistant to explore the world of code and computation while physically manipulating the parameters of that world so as to coerce the underlying physics to yield solutions to computational problems. When it come to running code, however, the apprentice can't violate the physics of the native computing hardware.

While the interface to the IDE can be adapted to suit the needs of the apprentice, i.e., it is fully differentiable, the IDE itself and the representations that it relies upon are not differentiable and hence not amenable to adaptation via gradient descent. Moreover, the internal representations that the apprentice uses to reason about the behavior of running programs is typical of connectionist representations in that does not exhibit crisp concept boundaries or depend on immutable semantic categories. This is not really any different from the way our brains relate to the physical world in which we struggle to survive. However, the world of the apprentice is in many ways even less forgiving than the world in which we inhabit.

The language interface that determines how the programmer and apprentice relate to one another and collaborate to solve problems is patterned after human-to-human interaction and hence is subject to all the advantages and inadequacies that we observe in human-to-human collaboration and the struggle that software engineers face in translating human needs and aspirations into reliable products and computing platforms. One solution to the problem of interpretability is to design failsafe systems that limit what the non-differentiable components are capable of doing just as we attempt to build control systems that limit what their human operators are able to do.

By the way, I'm confident that the sort of digital assistants based upon what we know about human brains implemented as artificial neural networks will be able to provide post hoc interpretations for their actions. However, if we do a good job of capturing human reasoning, I wouldn't give those interpretations any more credence than I do humans, since the AI systems will be learning their tricks from listening to humans. Perhaps if we want systems that behave more like us in those respects we admire but better in those that we abhor, then we will have to design very different ways of training these systems than we use to educate our own children and not expose them to the sort of unscrupulous politicians we seem to elect or the entrepreneurs and businessmen whose successes and excesses our youth seek to emulate.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsectional{Heterogeneity}

When the programmer tells the assistant to replace the name of a variable in one location in a program with the name of a variable in another location, the process starts with a contextually rich representation in the programmer’s brain corresponding to the activation of millions or billions of neurons in circuits distributed broadly throughout the cortex. This pattern of activation is compressed into a more compact representation used to generate a sequence of words uttered one at a time as if condensing out of a cloud of commingled thoughts in droplets or phrasal showers uttered in sudden bursts of words that are \emdash{} ignoring the intervening stages of speech production in the programmer and auditory processing in the assistant \emdash{} subsequently converted into activations in peripheral subnetworks of the assistant and quickly propagate to other subnetworks throughout the assistant’s neural-network architecture. 

The resulting activations insinuate fractal patterns of meaning into broadly distributed subnetworks of the apprentice subtly altering activity in some and substantially altering activity in others, contributing to the formation of another contextually rich representation in the apprentice’s brain. The imperative conveyed by the programmer’s tone of voice produces a quick response. The apprentice performs a sequence of well rehearsed steps that involve activating a sequence of patterns in the non-differentiable interface connecting the assistant to the integrated development environment. This sequence is produced by recurrent networks operating much like the programmer’s speech production circuits. The resulting patterns produce a sequence of unambiguous words \emdash{} requiring no additional context to interpret, that immediately produce the desired change and are displayed on the screen shared by the programmer and the apprentice.

What does this combination of expert-human biological computing, differentiable connectionist models and non-differentiable symbolic systems buy us? The human expert provides heuristic advice and technical guidance. The connectionist components enable natural language communication between human and machine and enable the system to discover and exploit structure in computer programs to facilitate code synthesis and reduce brute search. Finally, the symbolic components allow the connectionist components \emdash{} and, by extension, the human expert \emdash{} to directly engage with computers as prosthetic extensions in which compiling and running code is as natural as playing video games.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsectional{Resources}

Neil Rabinowitz's {\urlh{https://web.stanford.edu/class/cs379c/calendar_invited_talks/lectures/04/17/slides/Neil_Rabinowitz_CS379C_04-17-18.pdf}{presentation}} on learning a machine theory-of-mind model that relies on meta-learning to build mental models of the agents that it encounters from observations of their behaviour alone~\cite{RabinowitzetalCoRR-18}\footnote{%
%
  The abstract for Rabinowitz~\etal{}~\cite{RabinowitzetalCoRR-18}:
%
  \begin{quotation}
%
    Theory of mind (ToM; Premack and Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network \emdash{} a ToMnet \emdash{} which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test (Wimmer and Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system \emdash{} which autonomously learns how to model other agents in its world \emdash{} is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.
%
\end{quotation}}.

Greg Wayne's {\urlh{https://web.stanford.edu/class/cs379c/calendar_invited_talks/lectures/05/03/slides/Greg_Wayne_CS379C_05-03-18.pdf}{presentation}} on {\tt{MERLIN}} a method for prediction in environments corresponding to partially observable Markov decision processes in which memory formation is guided by predictive modeling~\cite{WayneetalCoRR-18}\footnote{%
%
  The abstract for Wayne~\etal{}~\cite{WayneetalCoRR-18}:
%
  \begin{quotation}
%
    Animals execute goal-directed behaviours despite the limited range and scope of their sensors. To cope, they explore environments and store memories maintaining estimates of important information that is not presently available. Recently, progress has been made with artificial intelligence (AI) agents that learn to perform tasks from sensory input, even at a human level, by merging reinforcement learning (RL) algorithms with deep neural networks, and the excitement surrounding these results has led to the pursuit of related ideas as explanations of non-human animal learning. However, we demonstrate that contemporary RL algorithms struggle to solve simple tasks when enough information is concealed from the sensors of the agent, a property called "partial observability". An obvious requirement for handling partially observed tasks is access to extensive memory, but we show memory is not enough; it is critical that the right information be stored in the right format. We develop a model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling. MERLIN facilitates the solution of tasks in 3D virtual reality environments for which partial observability is severe and memories must be maintained over long durations. Our model demonstrates a single learning agent architecture that can solve canonical behavioural tasks in psychology and neurobiology without strong simplifying assumptions about the dimensionality of sensory input or the duration of experiences.
%
  \end{quotation}}.

Oriol Vinyals' {\urlh{https://web.stanford.edu/class/cs379c/calendar_invited_talks/lectures/05/10/slides/Oriol_Vinyals_CS379C_05-10-18.pdf}{presentation}} on an approach for model-based plan construction, evaluation and execution applied to sequential decision making problems relying on a method of imagination-based forecasting~\cite{PascanuetalCoRR-17}\footnote{%
%
  The abstract for Pascanu~\etal{}~\cite{PascanuetalCoRR-17}:
%
  \begin{quotation}
%
   Conventional wisdom holds that model-based planning is a powerful approach to sequential decision-making. It is often very challenging in practice, however, because while a model can be used to evaluate a plan, it does not prescribe how to construct a plan. Here we introduce the "Imagination-based Planner", the first model-based, sequential decision-making agent that can learn to construct, evaluate, and execute plans. Before any action, it can perform a variable number of imagination steps, which involve proposing an imagined action and evaluating it with its model-based imagination. All imagined actions and outcomes are aggregated, iteratively, into a "plan context" which conditions future real and imagined actions. The agent can even decide how to imagine: testing out alternative imagined actions, chaining sequences of actions together, or building a more complex "imagination tree" by navigating flexibly among the previously imagined states using a learned policy. And our agent can learn to plan economically, jointly optimizing for external rewards and computational costs associated with using its imagination. We show that our architecture can learn to solve a challenging continuous control problem, and also learn elaborate planning strategies in a discrete maze-solving task. Our work opens a new direction toward learning the components of a model-based planning system and how to use them.  
%
  \end{quotation}}.

Devi Parikh's {\urlh{https://web.stanford.edu/class/cs379c/calendar_invited_talks/lectures/05/22/index.html}{public lectures}} on learning to conduct meaningful dialog with humans in natural, conversational language by grounding the conversation in shared visual experience, inferring its context from history~\cite{DasetalCVPR-17}\footnote{%
%
  The abstract for Das~\etal{}~\cite{DasetalCVPR-17}:
%
  \begin{quotation}
%
    We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in the image, infer its context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on \hmapprox{}120k images from COCO, with a total of \hmapprox{}1.2M dialog question-answer pairs. We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders \emdash{} Late Fusion, Hierarchical Recurrent Encoder and Memory Network \emdash{} and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Our dataset, code, trained models and visual chatbot are available {\urlh{https://visualdialog.org/}{here}}.
%
  \end{quotation}}.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
